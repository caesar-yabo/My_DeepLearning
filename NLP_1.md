NLP从入门到不放弃（一）
日期：2019-07-04 19:38浏览：23评论：0
       以前从来没接触过自然语言处理这块，对机器学习的了解还停留在西瓜书上；导师一开始的安排也是让我做数据。天道好轮回，所有以前欠下的债早晚都得还回来，项目需求必须的用自然语言处理了。找了思维导图看了看，是个十分庞大的体系，学习过程也一直在做笔记，灵机一动不如发成博客，分享一下学习路径和一个站在大门口的小白管中窥豹的心得吧。

```
   内容大部分应该都是各种csdn、知乎上的整理，可能比较杂乱无章，尽量会把参考链接都附上，供有兴趣的同事参考。
```

一、自然语言处理思维导图（来自知乎https://zhuanlan.zhihu.com/p/56802149）

```
   自然语言处理确实是一个非常庞大的世界，任何一点都值得深入研究~精力有限，项目需求是检索匹配，也就是文本相似度相关内容。接下来发现涉及到其他领域的知识点可能就不看了8
```

二、中文分词

```
   项目流程应当是  输入一个语句--------（进行分词、翻译、繁简转换、纠错等等处理）------->>计算词向量及句向量--------（与数据库进行匹配）-------->>得出匹配度较高的几个答案。不仅输入需要分词，词向量等计算也需要用到。考察了一下现在常见的中文分词，列举如下：

   1.盘古分词https://archive.codeplex.com/?p=pangusegment
          • 中文未登录词识别 --盘古分词可以对一些不在字典中的未登录词自动识别
          • 词频优先 --盘古分词可以根据词频来解决分词的歧义问题
          • 多元分词 --盘古分词提供多重输出解决分词粒度和分词精度权衡的问题
          • 中文人名识别 
          • 强制一元分词
          • 繁体中文分词
          • 同时输出简体和繁体
          • 中文词性输出 --盘古分词可以将以登录词的中文词性输出给用户，以方便用户做进一步处理。
          • 全角字符支持 --盘古分词可以识别全角的字母和数字
          • 停用词过滤---供一个 StopWord.txt 
          • 设置分词权值--未登录词、数字英文等等
   2.jieba分词 https://github.com/fxsjy/jieba
          • 支持三种分词模式：
          • 精确模式，试图将句子最精确地切开，适合文本分析；
          • 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
          • 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
          • 支持繁体分词
          • 支持自定义词典
   3.hanlp
   4.baidu lac
```

```
   功课做到这里产生一个小疑问，之前例会听分享，有项目是用hanlp进行分词，我们的项目要求输入会有大量的中英文混合文本，因此要求在能实现分词基础上还可以调整权值，不知道hanlp是否支持。这个小疑问还没来得及看的时候随口跟师父聊天，师父说分词技术已经非常成熟了，直接调用jieba就可以，这个小疑问就直接
```

解决了…





```
   今日收获：这种顺利的测评果然是没有意义的……
```

